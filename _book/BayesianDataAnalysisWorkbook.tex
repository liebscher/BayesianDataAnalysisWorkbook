\documentclass[]{book}
\usepackage{lmodern}
\usepackage{amssymb,amsmath}
\usepackage{ifxetex,ifluatex}
\usepackage{fixltx2e} % provides \textsubscript
\ifnum 0\ifxetex 1\fi\ifluatex 1\fi=0 % if pdftex
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
\else % if luatex or xelatex
  \ifxetex
    \usepackage{mathspec}
  \else
    \usepackage{fontspec}
  \fi
  \defaultfontfeatures{Ligatures=TeX,Scale=MatchLowercase}
\fi
% use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
% use microtype if available
\IfFileExists{microtype.sty}{%
\usepackage[]{microtype}
\UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\PassOptionsToPackage{hyphens}{url} % url is loaded by hyperref
\usepackage[unicode=true]{hyperref}
\hypersetup{
            pdftitle={Bayesian Data Analysis Workbook},
            pdfauthor={Alex Liebscher},
            pdfborder={0 0 0},
            breaklinks=true}
\urlstyle{same}  % don't use monospace font for urls
\usepackage{natbib}
\bibliographystyle{apalike}
\usepackage{longtable,booktabs}
% Fix footnotes in tables (requires footnote package)
\IfFileExists{footnote.sty}{\usepackage{footnote}\makesavenoteenv{long table}}{}
\usepackage{graphicx,grffile}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
\IfFileExists{parskip.sty}{%
\usepackage{parskip}
}{% else
\setlength{\parindent}{0pt}
\setlength{\parskip}{6pt plus 2pt minus 1pt}
}
\setlength{\emergencystretch}{3em}  % prevent overfull lines
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
\setcounter{secnumdepth}{5}
% Redefines (sub)paragraphs to behave more like sections
\ifx\paragraph\undefined\else
\let\oldparagraph\paragraph
\renewcommand{\paragraph}[1]{\oldparagraph{#1}\mbox{}}
\fi
\ifx\subparagraph\undefined\else
\let\oldsubparagraph\subparagraph
\renewcommand{\subparagraph}[1]{\oldsubparagraph{#1}\mbox{}}
\fi

% set default figure placement to htbp
\makeatletter
\def\fps@figure{htbp}
\makeatother

\usepackage{booktabs}

\title{Bayesian Data Analysis Workbook}
\author{Alex Liebscher}
\date{2020-07-17}

\begin{document}
\maketitle

{
\setcounter{tocdepth}{1}
\tableofcontents
}
\chapter{Introduction}\label{introduction}

Contained within this notebook is an approximate following of Bayesian
Data Analysis (3rd edition) by Gelman, Carlin, Stern, Dunson, Vehtari,
and Rubin (2020).

My goal is to follow the book, with as few jumps as necessary, and copy
their work, their writing, and their code. Through this, I hope to learn
how to apply Bayesian principles to the everyday data analysis problem.

Most of the text used in this workbook is paraphrased directly from the
original without shame. Their examples are sufficient and in many cases
I would do a disservice altering their language. This workbook is simply
an exercise to practice active reading, note-taking, and implementation.
It is an opportunity to learn something about Bayesian inference from a
collection of bright scholars to whom I trust to present a thorough,
captivating read of the subject.

I anticipate this to be highly interactive, which is why I've opted to
work directly in a \texttt{bookdown} format. Hopefully, some nice plots
will be created along the way.

Some code might be compared to
\href{https://github.com/avehtari/BDA_R_demos}{Aki Vehtari's}. Vehtari
also has published their
\href{https://github.com/avehtari/BDA_course_Aalto}{own lecture notes},
which could also be a helpful resource. Luckily too, it seems like most
of the data is hosted on Gelman's site - this should come in handy.

\section{Collaboration}\label{collaboration}

I am looking for a reading group to incrementally work through the
textbook and have discussions with. If you are looking to learn about
Bayesian inference, I hope you won't be shy and we can struggle
together. Email me at \texttt{alexliebscher0@gmail.com} and just say
you're interested, I'll figure out the rest.

Moreover, feel free to clone this repo for a quick start.

\chapter{Probability and Inference}\label{intro}

Bayesian data analysis may be broken down into three distinct steps:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Designing the full probability model
\item
  Conditioning on observed data
\item
  Evaluating the fit of the model
\end{enumerate}

A primary motivation for adopting a Bayesian framework is that it allows
us to interpret the statistical conclusions closer to our common-sense
human intuition. The central feature to Bayesian analysis is the direct
quantification of uncertainty.

In terms of notation, we define:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \(\theta\) as the unobservable vector quantities or population
  parameters of interest.

  \begin{enumerate}
  \def\labelenumii{\alph{enumii}.}
  \tightlist
  \item
    Example: the probabilities of survival under a control and a
    treatment for randomly chosen members of the population.
  \end{enumerate}
\item
  \(y\) as the observed data.

  \begin{enumerate}
  \def\labelenumii{\alph{enumii}.}
  \setcounter{enumii}{1}
  \tightlist
  \item
    Example: the known number of survivors and deaths in both the
    control and treatment groups.
  \end{enumerate}
\item
  \(\tilde{y}\) as unknown, but potentially observable, quanitites.

  \begin{enumerate}
  \def\labelenumii{\alph{enumii}.}
  \setcounter{enumii}{2}
  \tightlist
  \item
    Example: the outcome (survival or death) of an unseen patient
    similar to those already in the experiment.
  \end{enumerate}
\end{enumerate}

The \(y\) variables are called the ``outcomes'' and may be represented
as, e.g.~1 if patient \(i\) survives and 0 is patient \(i\) dies, so
that \(y\) takes the form of a vector. These values are considered
\emph{random} when making inferences because there is the possibility
they could have been the opposite outcome due to the sampling process or
the natural variation in the population.

For the time being, we consider the values of \(y\) to be independent
and identically distributed (iid).

It also common to have explanatory variables or covariates. This may
include age, previous health status, etc. \(X\) denotes this set of
\(k\) variables across all \(n\) observations.

\section{Bayesian inference}\label{bayesian-inference}

Conclusions in a Bayesian framework about our \(\theta\) (remember, the
unobservable quantities or population parameters), or our \(\tilde{y}\)
(our unknown, but possible, outcomes), stem from either
\(p(\theta | y)\) or \(p(\tilde{y} | y)\). This is to say that our
parameters, or our unknown outcomes, are identified through a
probability statement, conditional on our observed data.

Knowing that this is what we're chasing, we can introduce Bayes' rule:

\[
p(\theta, y) = p(\theta)p(y|\theta)
\]

In this equation, \(p(\theta)\) is called the \emph{prior distribution},
and \(p(y|\theta)\) is called the \emph{sampling distribution}.

Note that we can also write this as \(p(\theta, y) = p(y)p(\theta|y)\).
Therefore, together with the last result,

\[
p(\theta|y) = \frac{p(\theta,y)}{p(y)} = \frac{p(\theta)p(y|\theta)}{p(y)}
\]

where the \emph{prior predictive distribution} is
\(p(y) = \sum_\theta p(\theta)p(y|\theta)\) (or
\(p(y)=\int p(\theta)p(y|\theta)d\theta\) if \(\theta\) is continuous).

This is called the \emph{posterier density}. These formulas represent
the core of Bayesian statistics.

The distribuion of \(\tilde{y}\) is called the \emph{posterior
predictive distribution} and takes the form:

\[
p(\tilde{y}|y) = \int p(\tilde{y},\theta|y)d\theta = \int p(\tilde{y}|\theta,y)p(\theta|y)d\theta
\]

\subsection{Inference about a genetic
status}\label{inference-about-a-genetic-status}

A first example, with setup found under Section 1.4 pg. 8.

First, we set up the prior distribution:

\begin{quote}
Consider a woman who has an affected brother, which implies that her
mother must be a carrier of the hemophilia gene with one `good' and one
`bad' hemophilia gene. We are also told that her father is not affected;
thus the woman herself has a fifty-fifty chance of having the gene. The
unknown quantity of interest, the state of the woman, has just two
values: the woman is either a carrier of the gene (\(\theta\) = 1) or
not (\(\theta\) = 0). Based on the information provided thus far, the
prior distribution for the unknown \(\theta\) can be expressed simply as
P(\(\theta = 1\)) = P(\(\theta = 0\)) = \(\frac{1}{2}\).
\end{quote}

Second, we establish our data model and the likelihood formula:

There are two possible worlds here: first, the woman in question is
affected; second, the women in question is \emph{not} affected. Suppose
she has two sons, neither of whom are affected. The status of the two
sons is independent: one son's status does not affect the other's. They
both, however, rely on the mother's (unknown) status; they are
conditional upon her status. Thus, the two items of independent data
generate the following likelihood functions:

P(son\(_1\) unaffected, son\(_2\) unaffected
\(| \theta = 1) = (0.5)(0.5) = 0.25\)

P(son\(_1\) unaffected, son\(_2\) unaffected
\(| \theta = 0) = (1)(1) = 1\)

Third, we establish the posterior distribution:

Using Bayes' rule, we can now combine the information we know from the
data with our prior knowledge. If we let \(y\) = (son\(_1\) status,
son\(_2\) status), then:

\[
P(\theta = 1|y) = \frac{p(y|\theta = 1)P(\theta = 1)}{p(y|\theta = 1)P(\theta = 1) + p(y|\theta = 0)P(\theta = 0)}\\
= \frac{(0.25)(0.5)}{(0.25)(0.5) + (1.0)(0.5)} = \frac{0.125}{0.625} = 0.2
\]

A key aspect of Bayesian analysis is the ease at which we may add
additional data to the mix. For example, suppose the mother has a third
son. We don't need to recalculate the entire formula from where we
started, instead we can substitute the newly found posterior
distribution as the new data model (\(y\) = (son\(_1\) status, son\(_2\)
status, son\(_3\) status)):

\[
P(\theta = 1|y) = \frac{p(y|\theta = 1)P(\theta = 1)}{p(y|\theta = 1)P(\theta = 1) + p(y|\theta = 0)P(\theta = 0)}\\
= \frac{(0.2)(0.5)}{(0.2)(0.5) + (1.0)(0.8)} = \frac{0.1}{0.9} = 0.111
\]

Point of confusion: On pg. 9 the authors say: ``we use the previous
posterior distribution as the new prior distribution''. However,
according to their definition of the prior distribution \(p(\theta)\),
it would seem as though they're actually replacing the \emph{likelihood}
function \(p(y|\theta)\).

In any case, the new probability of the mother being a carrier is 11.1\%
given the status of her three sons.

\bibliography{book.bib,packages.bib}

\end{document}
